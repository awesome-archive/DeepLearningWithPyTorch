{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.draw import draw_digits\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, transforms"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 1, 28, 28)\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"digits = datasets.MNIST('data', download=True,\n                        transform=transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Lambda(lambda x: x.reshape(28*28))\n                        ]),\n                        target_transform=transforms.Compose([\n                            transforms.Lambda(lambda y: \n                                              torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n                        ])\n                     )"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"draw_digits(digits)"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"torch.cuda.is_available()"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Use the nn package to define our model and loss function.\nmodel = CNN()\nmodel = model.to(device)\n\ncost = torch.nn.BCELoss()\n\n# optimizer which Tensors it should update.\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = StepLR(optimizer, 3)\n\n# dataset!\ndataloader = DataLoader(digits, batch_size=64, num_workers=0, pin_memory=True)\n\nepochs = 10"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"for t in range(epochs):\n    print('\\nepoch {}, (lr: {:>.1e})'.format(t, scheduler.get_lr()[0]))\n    print('-------------------------------')\n    for batch, (X, Y) in enumerate(dataloader):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        pred = model(X)\n        loss = cost(pred, Y)\n        loss.backward()\n        optimizer.step()\n        \n        if batch % 100 == 0:\n            print('loss: {:>10f}  [{:>5d}/{:>5d}]'.format(loss.item(), batch * len(X), len(dataloader.dataset)))\n            \n    # step scheduler\n    scheduler.step()"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"# Does it work???"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"test_data = datasets.MNIST('data', train=False, download=True,\n                        transform=transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Lambda(lambda x: x.reshape(28*28))\n                        ]),\n                        target_transform=transforms.Compose([\n                            transforms.Lambda(lambda y: \n                                              torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n                        ])\n                     )\ntest_loader = DataLoader(digits, batch_size=64, num_workers=0, pin_memory=True)"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"model.eval()\ntest_loss = 0\ncorrect = 0\nwith torch.no_grad():\n    for batch, (X, Y) in enumerate(test_loader):\n        X, Y = X.to(device), Y.to(device)\n        pred = model(X)\n\n        test_loss += cost(pred, Y).item()\n        correct += (pred.argmax(1) == Y.argmax(1)).type(torch.float).sum().item()\n\ntest_loss /= len(dataloader.dataset)\ncorrect /= len(dataloader.dataset)\nprint('Test Error:')\nprint('acc: {:>0.1f}%, avg loss: {:>8f}'.format(100*correct, test_loss))"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"len(test_data)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"# Saving Things!"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"import torch.onnx as onnx\n\n# create dummy variable to traverse graph\nx = torch.randint(255, (1, 28*28), dtype=torch.float).to(device) / 255\nonnx.export(model, x, 'superfile.onnx')\nprint('Saved onnx model to \"superfile.onnx\"')"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}