{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.draw import draw_squares\nfrom utils.square import SquareDataset\nfrom torch.utils.data import DataLoader\nimport utils.viz as torchviz\ntorchviz.init()"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"class LinearModel(nn.Module):\n    def __init__(self, x, y):\n        super(LinearModel, self).__init__()\n        self.layer1 = nn.Linear(x, y)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        return x"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"squares = SquareDataset(60000)\nfor i in range(5):\n    print(squares[i])"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Using the not-so-smart Model"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"model = LinearModel(9, 3)\nX = squares[4][0].reshape(-1, 9)/255\nprint(X)\nprint(model(X))"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"model.layer1.weight, model.layer1.bias"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Looking at the gradients!"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"cost = torch.nn.MSELoss()\nY = squares[4][1].reshape(-1, 3)\nloss = cost(model(X), Y)\nprint(loss)"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"torchviz.draw(loss)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"# Optimizing All Teh Things!"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"# Use the nn package to define our model and loss function.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LinearModel(9, 3)\nmodel = model.to(device)\n\ncost = torch.nn.MSELoss()\n\n# optimizer which Tensors it should update.\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# dataset!\ndataloader = DataLoader(squares, batch_size=128)\n\nepochs = 20"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"# The Optimization Loop"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"for t in range(epochs):\n    for batch, (X, Y) in enumerate(dataloader):\n        X, Y = X.to(device) / 255, Y.to(device)\n        optimizer.zero_grad()\n        pred = model(X)\n        loss = cost(pred, Y)\n        loss.backward()\n        optimizer.step()\n\n    print('l: {:>8f}, (e {:>3})'.format(loss.item(), t))"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"print(\"W's and b's:\")\nfor p in model.parameters():\n    print(p)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"# Trying it out (inference)"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}